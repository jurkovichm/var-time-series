---
title: "Vector Autoregressive Models"
subtitle: "Time Series Final | 'Flavor 2'"
author: "Marko Jurkovich & Matt Zacharski"
date: "7 June 2021"
output: pdf_document
abstract: "This is our abstract, blah blah blah"
keywords: "keyword1, keyword2" 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(vars)
library(lmtest)
library(forecast)


# na 'resolver'. turns na's into previous day's close value

na2lag1 <- function(x) {
  i <- 1
  while (i <= length(x)) {
    if (is.na(x[i])) {
      x[i] <- x[i - 1]
    }
    i <- i + 1
  }
  return(x)
}
```


\newpage

### Introduction

The vector autoregressive model (VAR) is a time series model that is typically used for the analysis of multivariate time series. Consequently, VAR models are particularly useful in fields where several time series may affect one another. For example, a researcher may be interested in exploring the relationship between several foreign exchange rates overtime or the trends of two countries' GDPs. While the fields of economics and finance may be initial fields that come to mind when discussing VAR models, the natural sciences also use VAR models to analysis and forecast things such as rainfall while also considering humidity, temperature, and other factors. 

Like the name suggests, VAR models are an extension of the univariate autoregressive model. Similar to the univariate autoregressive model, each variable's equation includes its own lagged values. However, the VAR model goes a step further and also includes lagged values of other time series in each variable's equation. Including this extra information allows VAR models to often make more accurate predictions than the general AR model, justifying their existence and use. It is worth noting that several related models have been developed over the past few decades. Namely, structural vector autoregressive models, vector error correction models, and structural vector error correction models. While these models are outside the scope of this paper, it is useful to know they attempt to improve the VAR model by accounting for concepts such as cointegration or some type of shock. 

In this paper, we will discuss the form, estimation, diagnostics, and forecasting process of VAR models. To highlight these methods, we have included a brief analysis of three cryptocurrency time series. Additionally, this paper will only deal with the selection and forecasting of stationary VAR models. 


### Model

The basic structure of a VAR model is quite similar to that of an AR model, however there are two fundamental differences. The first is that there are multiple equations, one for each item in the vector of time series considered. Secondly, rather than have a time point $x_t$ of a given time series being regressed on its own lags, it is also regressed on the lags of the other time series. The order of the model (number of lags) is typically indicated by the *p* in VAR(*p*).

There are three main forms of VAR models: reduced form, recursive, and structural. The reduced form defines each variable in the vector of time series as the function of its own lags and the lags of all other variables, with an error term. The error terms from each equation can theoretically be correlated, *but only within the same time period*. Correlation across equations across different time periods would imply autocorrelated errors within a single time series, which is not an assumption of the model.

Recursive VAR models are constructed so that the error terms for a variable are uncorrelated to the prior variables' errors. This is done by adding the  preceding variables' current values to the typical VAR equation. 

The structural VAR models are distinguished in that they make assumptions of the "causal structure" of the the data allow shocks to be indentified. These shocks would otherwise be incorporated into error terms in recursive and reduced models.

A reduced VAR(1) model with  three variables can be modeled as such:

\begin{equation}
\begin{aligned}
x_{t,1} = \alpha_1 + \beta_{1,1}x_{t-1,1} +\beta_{1,2}x_{t-1,2} + \beta_{1,3}x_{t-1,3}+\epsilon_{t,1} \\
x_{t,2} = \alpha_2 + \beta_{2,1}x_{t-1,1} +\beta_{2,2}x_{t-1,2} + \beta_{2,3}x_{t-1,3}+\epsilon_{t,2} \\
x_{t,1} = \alpha_3 + \beta_{3,1}x_{t-1,1} +\beta_{3,2}x_{t-1,2} + \beta_{3,3}x_{t-1,3}+\epsilon_{t,3}
\end{aligned}
\end{equation}

We can generalize the previous equations for a VAR(*p*) model with *k* variables using matrix algebra:

\begin{equation}
\begin{bmatrix}x_{1,t} \\ x_{2,t}\\ \vdots \\ x_{k,t}\end{bmatrix} = 
\begin{bmatrix} \alpha_{1}^1 & \alpha_{1}^2 \\ \alpha_{2} & \alpha_{2}^2\\ \vdots & \vdots  \\ \alpha_{k}^1 & \alpha_k^2\end{bmatrix} \begin{bmatrix} 1 \\ t \end{bmatrix}+
\begin{bmatrix}
\beta_{1,1}^1 & \cdots & \beta_{1,k}^1\\
\beta_{2,1}^1 & \cdots & \beta_{2,k}^1\\
\vdots& \ddots& \vdots\\
\beta_{k,1}^1& \cdots & \beta_{k,k}^1
\end{bmatrix}
\begin{bmatrix}x_{t-1,1} \\ x_{t-1,2}\\ \vdots \\ x_{t-1,k}\end{bmatrix}
+ \cdots +
\begin{bmatrix}
\beta_{1,1}^p & \cdots & \beta_{1,k}^p\\
\beta_{2,1}^p & \cdots & \beta_{2,k}^p\\
\vdots& \ddots& \vdots\\
\beta_{k,1}^p & \cdots & \beta_{k,k}^p
\end{bmatrix}
\begin{bmatrix}y_{t-p,1} \\ y_{t-p,2}\\ \vdots \\ y_{t-p,k}\end{bmatrix}
+ \begin{bmatrix}\epsilon_{t,1} \\ \epsilon_{t,2}\\ \vdots \\ \epsilon_{t,3}\end{bmatrix}
\end{equation}

or, abstracting the matrices

\begin{equation}
\bf{x}_{t} =Au + B^1x_{t-1} +...+B^px_{t-p} +\epsilon_t 
\end{equation}

where $\bf{x}_{t}$ is an (k x 1) vector of time series variables, and $\bf{x}_{t-i}$ are the vectors of the time series variables at various lags determined by *p*. $\bf{A}$ is the coefficient matrix on the constant and trend term (both optional) represented in $\bf{u}$. $\bf B^ix_{t-1}$ are the (k x k) coefficient matrices; there are *p* of these matrices dependent on the order of the VAR(p) model. $\bf \epsilon_t$ is the vector of error terms.

### Estimation

VAR models fall into the category of Seemingly Unrealted Regression (SUR) models, which, without diving into too much detail, allows the combination multivariate systems into a conceptially simpler notation. The VAR case of SUR models in one in which all the explanatory variables (but not the coefficients, importantly) are indentical across equations. While SUR models often use Generalized Least Squares to estimate the equations, in the case of VAR models OLS is theoretically just as efficient at estimating each equation individually.

An extremely important consideration before estimating VAR models however, is parameter selection and extent. As both variables and lags are added to VAR models, the number of coefficients increases quadratically. For example, a "nine-variable, four-lag VAR has 333 unknown coefficients" (Stock 110).

- interpretation of coeffs

- asymptotically valid t-tests on individual coefficients may be constructed in the usual way

### Diagnostics and Lag Length Selection

The diagnostic testing for VAR models is similar to the diagnostic testing for AR models except with a few extra tests to consider. **Idk if we want to include information criteria in the diagnostic section or not but it seems like something worth considering**  

However, first we will go over the lag length selection process. The question of how many lag terms to include in a VAR model is an important because it plays a substantial role in the quality of forecasts the model will make. The common approach to select this number is to fit VAR(*p*) models with lag lengths from *p* = 0,...,$p_{max}$ and choose the *p* with that minimizes the considered information criteria. Typically, Akaike, Bayesian, Hannan-Quinn, and forecast prediction error information criterion are considered. Model selection criteria for VAR(*p*) models has the form

\begin{equation}
\begin{aligned}
IC(p)=ln|\tilde \sum(p)|+c_T*\varphi(n,p)
\end{aligned}
\end{equation}

where $\tilde \sum(p) = T^-1 \sum^{T}_{t=1} \hat \epsilon_t \hat \epsilon^\prime_t$ represents the residual covariance matrix without a degrees of freedom correction, $c_T$ is a sequence indexed by the sample size T, and $\varphi(n,p)$ is a penalty function to penalize large VAR(*p*) models.*neccessary to write out the individual equations for each criteria?? idk*. Each criterion has its benefits and drawbacks. However, these are not the focus on this paper but it is generally best to consider several criteria when determining the optimal lag length selection. 

When determining the quality of the fit of a certain VAR model, it is useful to consider the ACF and PACFs of the residuals. Similar to evaluating an AR model, we are looking for ACF and PACF plots that show no Considering autocorrealtion within the residuals. Unlike with the univariate AR models, we must also consider cross correaltion plots. Considering the ACF plots of the residuals for each set of residuals of all of the variables in a VAR model is important in order to obsere any cross-correlations between variables indicating that your model might not be capturing the relationship between those variables and time sufficiently. 

To test for heteroscedasticity within errors it is common to use an ARCH-LM test. However, this test relies on a complicated regression that is outside the scope of this paper. Similarly, the technical details of the normality test are also outside the scope of this paper but the gist of it is that researchers should use Jarque-Bera normality tests to test for normality of residuals of either each individual equation or the normality of all the equations' residuals using a variance-covariance matrix for the centered residuals.

It is also wise to test for autocorrelation within the residuals of VAR(*p*) models to avoid standard errors being too small or T-statistics that are too large. Do to this within a VAR(*p*) setting we can conduct a Portmanteau test of the form

\begin{equation}
\begin{aligned}
Q_h = T \sum_{j = 1}^h tr(\hat{C}_j'\hat{C}_0^{-1}\hat{C}_j\hat{C}_0^{-1}) \quad
\end{aligned}
\end{equation}

where $\hat{C}_i = \frac{1}{T}\sum_{t = i + 1}^T \bf{\hat{u}}_t \bf{\hat{u}}_{t - i}'^\top$ and the test statistics $Q_h$ has an approximate $\chi^2(K^2(h - n^*))$ distribution. $n^*$ refers to the number of non-deterministic terms in the specified VAR(p) model.



stability test 

granger test for causality? not going to include this because it seems a little outside the scope...


### Forecasting 

Forecasting in VAR is somewhat straightforward and similar to typical AR models. A one-step ahead forecast made at time *t+1* can be expressed as:

\begin{equation}
\bf{x}_{t+1|t} =Au + B^1x_t +...+B^px_{t-p+1}
\end{equation}

Predicting more than one step ahead is done recursively in what can be referred to as the chain-rule of forecasting:

\begin{equation}
\bf{x}_{t+1|t} =Au + B^1x_t +...+B^px_{t-p+1}
\end{equation}

- what happens to SEs

### Data Example

(matt)

1. clean code...
2. somehow highlight the modeling process in this paper? idk ....


### Discussion

seems like there's several areas of analysis when it comes to VAR models -- forecasting then observing the relationship between variables etc 

### References

Pfaff, Bernhard. VAR, SVAR and SVEC Models: Implementation Within R Package vars. Journal of Statistical Software 27(4), 2008. https://cran.r-project.org/web/packages/vars/vignettes/vars.pdf.

PennState Eberly College of Science. Vector autoregressive models VAR(p) models.
https://online.stat.psu.edu/stat510/lesson/11/11.2.

Shumway, Robert H., and David S. Stoffer. Time Series Analysis and Its Applications. Springer Texts in Statistics, Springer International Publishing, 2017, pp. 273–279. https://link.springer.com/content/pdf/10.1007%2F978-3-319-52452-8.pdf.

Stock, James H., and Mark W. Watson. “Vector Autoregressions.” Journal of Economic Perspectives, vol. 15, no. 4, American Economic Association, Nov. 2001, pp. 101–15. Crossref, doi:10.1257/jep.15.4.101.

### Code Appendix

```{r}
# reading in data

btc <- read_csv("BTC-USD.csv")
eth <- read_csv("ETH-USD.csv")
doge <- read_csv("DOGE-USD.csv")
eth <- eth[-1827, ] # matching length
```


```{r}
# creating closing price dataframe

closing_prices <- as.data.frame(cbind(btc$Close, eth$Close, doge$Close))

closing_prices$Date <- btc$Date

colnames(closing_prices) <- c("BTC", "ETH", "DOGE", "Date")

plot(closing_prices$Date, closing_prices$BTC, type = "l")
lines(closing_prices$Date, closing_prices$ETH, type = "l", col = "red")
lines(closing_prices$Date, closing_prices$DOGE, type = "l", col = "dark green")
```


```{r}
# creating scaled data dataframe

btc_scale <- scale(closing_prices$BTC)
doge_scale <- scale(closing_prices$DOGE)
eth_scale <- scale(closing_prices$ETH)

scaled_data <- data.frame(
  Date = closing_prices$Date,
  scaled_bth = btc_scale[, 1],
  scaled_eth = eth_scale[, 1],
  scaled_doge = doge_scale[, 1]
)

plot(scaled_data$Date, scaled_data$scaled_bth, type = "l")
lines(scaled_data$Date, scaled_data$scaled_eth, type = "l", col = "red")
lines(scaled_data$Date, scaled_data$scaled_doge, type = "l", col = "dark green")
```


```{r}
# creating scaled log dataframe
# resolving NAs
# na2lag1(c(closing_prices$BTC,closing_prices$DOGE,closing_prices$ETH))

Date <- closing_prices$Date
sl_btc <- scale(log((closing_prices$BTC)))[, 1]
sl_doge <- scale(log(as.numeric(closing_prices$DOGE)))[, 1]
sl_eth <- scale(log(as.numeric(closing_prices$ETH)))[, 1]
sl_data <- data.frame(Date, sl_btc, sl_doge, sl_eth)

plot(sl_data$Date, sl_data$sl_btc, type = "l")
lines(sl_data$Date, sl_data$sl_eth, type = "l", col = "red")
lines(sl_data$Date, sl_data$sl_doge, type = "l", col = "dark green")
```


```{r}
# differenced scaled (unlogged) data plot
plot(scaled_data$Date[c(-1, -2)], diff(scaled_data$scaled_bth, 2), type = "l")
lines(scaled_data$Date[c(-1, -2)], diff(scaled_data$scaled_eth, 2), type = "l", col = "red")
lines(scaled_data$Date[c(-1, -2)], diff(scaled_data$scaled_doge, 2), type = "l", col = "dark green")
```

```{r}
# varselect for scaled & logged data

slvar <- VARselect(na.omit(sl_data[,-1]), lag.max = 100, type = "both")
slvar$selection
plot(slvar$criteria[1,], type = 'l')
lines(slvar$criteria[2,], type = 'l', col = "red")
```



```{r}
# prepping data for vars package
scaled_data_clean <- scaled_data[, -1] %>% na.omit()
og_data_clean <- na.omit(closing_prices)
og_data_clean <- og_data_clean[, 4]

# fitting var model
fitvar <- VAR(scaled_data_clean, p = 2, type = "both")

summary(fitvar)
acf(residuals(fitvar))

# UNSURE; TO BE LABELED
ts_obj <- as.ts(scaled_data_clean)
var12 <- VAR(ts_obj, p = 12, type = "const")
serial.test(var12, lags.pt = 12, type = "PT.asymptotic")

# forecasts

forecast(var12) %>%
  autoplot() + xlab("Year")

# should we do an ADF test?? 

# granger test for causality COMMENTED OUT RN

grangertest(sl_btc ~ sl_doge, data = sl_data, order = 2)

#grangertest(BTC ~ ETH, data = og_data_clean)

  #grangertest(DOGE ~ BTC, order = 50, data = og_data_clean)
```


```{r}

# removing NAs 
sl_data$sl_btc<-na2lag1(sl_data$sl_btc)
sl_data$sl_doge<-na2lag1(sl_data$sl_doge)
sl_data$sl_eth<-na2lag1(sl_data$sl_eth)


# using results of VARselect to fit two models than compare them
fitvar_p9 <- VAR(sl_data[,-1], p = 9, type = "both")
fitvar_p1 <- VAR(sl_data[,-1], p = 1, type = "both")

summary(fitvar_p9)
summary(fitvar_p1)

# diagnostic tests 

# Asymptotic Portmanteau test

# we fail to find evidence of autocorrelation in the residuals of the VAR(9) model at the default of 16 lags, but if we do 40 we find significant evidence of autocorrealtion, might be a problem??
serial.test(fitvar_p9, lags.pt = 50)

# we find evidence of autocorrelation in the residuals of the VAR(1) model
serial.test(fitvar_p1, lags.pt = 40)


# Breusch-Godfrey test

# same reversial present for the BG test...
serial.test(fitvar_p9,lags.bg = 40, type = 'BG')


serial.test(fitvar_p1,lags.bg = 40, type = 'BG')

# more diagnostics 
stablity_test_p9 <-stability(fitvar_p9, type = c("OLS-CUSUM"))
stablity_test_p1 <-stability(fitvar_p1, type = c("OLS-CUSUM"))

plot(stablity_test_p9)
plot(stablity_test_p1)


# Jarque–Bera test is a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution
p9_normal <- normality.test(fitvar_p9)

#plot(p9_normal)


p1_normal <- normality.test(fitvar_p1)

#plot(p1_normal)

# Portmanteau Q and test for the null hypothesis that the residuals of a ARIMA model are homoscedastic
arch.test(fitvar_p9)
arch.test(fitvar_p1)


#making some forecasts

#need to refit model using slightly differently formatted data 
sl_data_as_ts <- as.ts(sl_data[,-1])
fitvar_p9 <- VAR(sl_data_as_ts, p = 9, type = "both")

#forecast(fitvar_p9, h = 365) %>%
  #autoplot() + xlab("Year")

fitvar_p1 <- VAR(sl_data_as_ts, p = 1, type = "both")

#forecast(fitvar_p1, h = 365) %>%
 # autoplot() + xlab("Year")

#irf stuff

#plot(irf(fitvar_p9))


#plot(fitvar_p9)

```


