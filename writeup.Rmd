---
title: "Vector Autoregressive Models"
author: "Marko Jurkovich & Matt Zacharski"
date: "6/3/2021"
output: pdf_document
abstract: "This is our abstract, blah blah blah"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(vars)
library(lmtest)
library(forecast)


# na 'resolver'. turns na's into previous day's close value

na2lag1 <- function(x) {
  i <- 1
  while (i <= length(x)) {
    if (is.na(x[i])) {
      x[i] <- x[i - 1]
    }
    i <- i + 1
  }
  return(x)
}
```


\newpage

### Introduction

(matt)
- Motivated by the goal of predicting a time series via a correlated partner time series, we will investigate Vector Autoregressive models (AKA VAR(p) models). 

- Why VAR models exist, what they're better at than AR models. In predictions they don't just include a variable at time t, but take all previous observations into account.

### Model

(marko)

The basic structure of a VAR model is quite similar to that of an AR model, however there are two fundamental differences. The first is that there are multiple equations, one for each item in the vector of time series considered. Secondly, rather than have a time point $x_t$ of a given time series being regressed on its own lags, it is also regressed on the lags of the other time series. The order of the model (number of lags) is typically indicated by the *p* in VAR(*p*).

There are three main forms of VAR models: reduced form, recursive, and structural. The reduced form defines each variable in the vector of time series as the function of its own lags and the lags of all other variables, with an error term. The error terms from each equation can theoretically be correlated, *but only within the same time period*. Correlation across equations across different time periods would imply autocorrelated errors within a single time series, which is not an assumption of the model.

Recursive VAR models are constructed so that the error terms for a variable are uncorrelated to the prior variables' errors. This is done by adding the  preceding variables' current values to the typical VAR equation. 

The structural VAR models are distinguished in that they make assumptions of the "causal structure" of the the data allow shocks to be indentified. These shocks would otherwise be incorporated into error terms in recursive and reduced models.

A reduced VAR(1) model with  three variables can be modeled as such:

$$x_{t,1} = \alpha_1 + \beta_{1,1}x_{t-1,1} +\beta_{1,2}x_{t-1,2} + \beta_{1,3}x_{t-1,3}+\epsilon_{t,1}$$
$$x_{t,2} = \alpha_2 + \beta_{2,1}x_{t-1,1} +\beta_{2,2}x_{t-1,2} + \beta_{2,3}x_{t-1,3}+\epsilon_{t,2}$$
$$x_{t,1} = \alpha_3 + \beta_{3,1}x_{t-1,1} +\beta_{3,2}x_{t-1,2} + \beta_{3,3}x_{t-1,3}+\epsilon_{t,3}$$

We can generalize the previous equations for a VAR(*p*) model with *k* variables using matrix algebra:
\begin{math}
\begin{bmatrix}x_{1,t} \\ x_{2,t}\\ \vdots \\ x_{k,t}\end{bmatrix} = 
\begin{bmatrix} \alpha_{1}^1 & \alpha_{1}^2 \\ \alpha_{2} & \alpha_{2}^2\\ \vdots & \vdots  \\ \alpha_{k}^1 & \alpha_k^2\end{bmatrix} \begin{bmatrix} 1 \\ t \end{bmatrix}+
\begin{bmatrix}
\beta_{1,1}^1& \beta_{1,2}^1 & \cdots & \beta_{1,k}^1\\
\beta_{2,1}^1&\beta_{2,2}^1 & \cdots & \beta_{2,k}^1\\
\vdots& \vdots& \ddots& \vdots\\
\beta_{k,1}^1&\beta_{k,2}^1 & \cdots & \beta_{k,k}^1
\end{bmatrix}
\begin{bmatrix}x_{t-1,1} \\ x_{t-1,2}\\ \vdots \\ x_{t-1,k}\end{bmatrix}
+ \cdots +
\begin{bmatrix}
\beta_{1,1}^p&\beta_{1,2}^p & \cdots & \beta_{1,k}^p\\
\beta_{2,1}^p&\beta_{2,2}^p & \cdots & \beta_{2,k}^p\\
\vdots& \vdots& \ddots& \vdots\\
\beta_{k,1}^p&\beta_{k,2}^p & \cdots & \beta_{k,k}^p
\end{bmatrix}
\begin{bmatrix}y_{t-p,1} \\ y_{t-p,2}\\ \vdots \\ y_{t-p,k}\end{bmatrix}
+ \begin{bmatrix}\epsilon_{t,1} \\ \epsilon_{t,2}\\ \vdots \\ \epsilon_{t,3}\end{bmatrix}
\end{math}

or, abstracting the matrices

$$\bf{x}_{t} =Au + B^1x_{t-1} +...+B^px_{t-p} +\epsilon_t $$ 

where $$\bf{x}_{t}$$ is an (k x 1) vector of time series variables, and $\bf{x}_{t-i}$ are the vectors of the time series variables at various lags determined by *p*. $\bf{A}$ is the coefficient matrix on the constant and trend term (both optional) represented in $\bf{u}$. $\bf B^ix_{t-1}$ are the (k x k) coefficient matrices; there are *p* of these matrices dependent on the order of the VAR(p) model. $\bf \epsilon_t$ is the vector of error terms.

### Estimation

(marko)
- how its estimed

- "adding variables to the VAR creates complications, because the number of VAR parameters increases as the square of the number of variables: a nine-variable, four-lag VAR has 333 unknown coefficients" https://pubs.aeaweb.org/doi/pdf/10.1257/jep.15.4.101

- interpretation of coeffs

### Diagnostics
(matt)

### Forecasting 

(marko)

Forecasting in VAR is somewhat straightforward and similar to typical AR models. A 1-step ahead forecast made at time *t+1* can be expressed as:

$$\bf{x}_{t+1|t} =Au + B^1x_t +...+B^px_{t-p+1} $$


- recusive h-step ahead

### Data Example
(matt)

### Discussion

### References

Pfaff, Bernhard. VAR, SVAR and SVEC Models: Implementation Within R Package vars. Journal of Statistical Software 27(4), 2008. https://cran.r-project.org/web/packages/vars/vignettes/vars.pdf.

	PennState Eberly College of Science. Vector autoregressive models VAR(p) models.
https://online.stat.psu.edu/stat510/lesson/11/11.2.

Shumway, Robert H., and David S. Stoffer. Time Series Analysis and Its Applications. Springer Texts in Statistics, Springer International Publishing, 2017, pp. 273–279. https://link.springer.com/content/pdf/10.1007%2F978-3-319-52452-8.pdf.

### Code Appendix

```{r}
# reading in data

btc <- read_csv("BTC-USD.csv")
eth <- read_csv("ETH-USD.csv")
doge <- read_csv("DOGE-USD.csv")
eth <- eth[-1827, ] # matching length
```


```{r}
# creating closing price dataframe

closing_prices <- as.data.frame(cbind(btc$Close, eth$Close, doge$Close))

closing_prices$Date <- btc$Date

colnames(closing_prices) <- c("BTC", "ETH", "DOGE", "Date")

plot(closing_prices$Date, closing_prices$BTC, type = "l")
lines(closing_prices$Date, closing_prices$ETH, type = "l", col = "red")
lines(closing_prices$Date, closing_prices$DOGE, type = "l", col = "dark green")
```


```{r}
# creating scaled data dataframe

btc_scale <- scale(closing_prices$BTC)
doge_scale <- scale(closing_prices$DOGE)
eth_scale <- scale(closing_prices$ETH)

scaled_data <- data.frame(
  Date = closing_prices$Date,
  scaled_bth = btc_scale[, 1],
  scaled_eth = eth_scale[, 1],
  scaled_doge = doge_scale[, 1]
)

plot(scaled_data$Date, scaled_data$scaled_bth, type = "l")
lines(scaled_data$Date, scaled_data$scaled_eth, type = "l", col = "red")
lines(scaled_data$Date, scaled_data$scaled_doge, type = "l", col = "dark green")
```


```{r}
# creating scaled log dataframe
# resolving NAs
# na2lag1(c(closing_prices$BTC,closing_prices$DOGE,closing_prices$ETH))

Date <- closing_prices$Date
sl_btc <- scale(log((closing_prices$BTC)))[, 1]
sl_doge <- scale(log(as.numeric(closing_prices$DOGE)))[, 1]
sl_eth <- scale(log(as.numeric(closing_prices$ETH)))[, 1]
sl_data <- data.frame(Date, sl_btc, sl_doge, sl_eth)

plot(sl_data$Date, sl_data$sl_btc, type = "l")
lines(sl_data$Date, sl_data$sl_eth, type = "l", col = "red")
lines(sl_data$Date, sl_data$sl_doge, type = "l", col = "dark green")
```


```{r}
# differenced scaled (unlogged) data plot
plot(scaled_data$Date[c(-1, -2)], diff(scaled_data$scaled_bth, 2), type = "l")
lines(scaled_data$Date[c(-1, -2)], diff(scaled_data$scaled_eth, 2), type = "l", col = "red")
lines(scaled_data$Date[c(-1, -2)], diff(scaled_data$scaled_doge, 2), type = "l", col = "dark green")
```

```{r}
# varselect for scaled & logged data

slvar <- VARselect(na.omit(sl_data[,-1]), lag.max = 100, type = "both")
slvar$selection
plot(slvar$criteria[1,], type = 'l')
lines(slvar$criteria[2,], type = 'l', col = "red")
```



```{r}
# prepping data for vars package
scaled_data_clean <- scaled_data[, -1] %>% na.omit()
og_data_clean <- na.omit(closing_prices)
og_data_clean <- og_data_clean[, 4]

# fitting var model
fitvar <- VAR(scaled_data_clean, p = 2, type = "both")

summary(fitvar)
acf(residuals(fitvar))

# UNSURE; TO BE LABELED
ts_obj <- as.ts(scaled_data_clean)
var12 <- VAR(ts_obj, p = 12, type = "const")
serial.test(var12, lags.pt = 12, type = "PT.asymptotic")

# forecasts

forecast(var12) %>%
  autoplot() + xlab("Year")

# should we do an ADF test?? 

# granger test for causality COMMENTED OUT RN

grangertest(sl_btc ~ sl_doge, data = sl_data, order = 2)

#grangertest(BTC ~ ETH, data = og_data_clean)

  #grangertest(DOGE ~ BTC, order = 50, data = og_data_clean)
```


```{r}

# removing NAs 
sl_data$sl_btc<-na2lag1(sl_data$sl_btc)
sl_data$sl_doge<-na2lag1(sl_data$sl_doge)
sl_data$sl_eth<-na2lag1(sl_data$sl_eth)


# using results of VARselect to fit two models than compare them
fitvar_p9 <- VAR(sl_data[,-1], p = 9, type = "both")
fitvar_p1 <- VAR(sl_data[,-1], p = 1, type = "both")

summary(fitvar_p9)
summary(fitvar_p1)

# diagnostic tests 

# Asymptotic Portmanteau test

# we fail to find evidence of autocorrelation in the residuals of the VAR(9) model at the default of 16 lags, but if we do 40 we find significant evidence of autocorrealtion, might be a problem??
serial.test(fitvar_p9, lags.pt = 50)

# we find evidence of autocorrelation in the residuals of the VAR(1) model
serial.test(fitvar_p1, lags.pt = 40)


# Breusch-Godfrey test

# same reversial present for the BG test...
serial.test(fitvar_p9,lags.bg = 40, type = 'BG')


serial.test(fitvar_p1,lags.bg = 40, type = 'BG')

# more diagnostics 
stablity_test_p9 <-stability(fitvar_p9, type = c("OLS-CUSUM"))
stablity_test_p1 <-stability(fitvar_p1, type = c("OLS-CUSUM"))

plot(stablity_test_p9)
plot(stablity_test_p1)


# Jarque–Bera test is a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution
p9_normal <- normality.test(fitvar_p9)

plot(p9_normal)


p1_normal <- normality.test(fitvar_p1)

plot(p1_normal)

# Portmanteau Q and test for the null hypothesis that the residuals of a ARIMA model are homoscedastic
arch.test(fitvar_p9)
arch.test(fitvar_p1)


#making some forecasts

#need to refit model using slightly differently formatted data 
sl_data_as_ts <- as.ts(sl_data[,-1])
fitvar_p9 <- VAR(sl_data_as_ts, p = 9, type = "both")

forecast(fitvar_p9, h = 365) %>%
  autoplot() + xlab("Year")

fitvar_p1 <- VAR(sl_data_as_ts, p = 1, type = "both")

forecast(fitvar_p1, h = 365) %>%
  autoplot() + xlab("Year")

#irf stuff

plot(irf(fitvar_p9))

```


