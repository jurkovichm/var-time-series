---
title: "Vector Autoregressive Models"
author: "Marko Jurkovich & Matt Zacharski"
date: "6/3/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(vars)
library(lmtest)
library(forecast)


# na 'resolver'. turns na's into previous day's close value

na2lag1 <- function(x) {
  i <- 1
  while (i <= length(x)) {
    if (is.na(x[i])) {
      x[i] <- x[i - 1]
    }
    i <- i + 1
  }
  return(x)
}
```

### Introduction

(matt)
- Motivated by the goal of predicting a time series via a correlated partner time series, we will investigate Vector Autoregressive models (AKA VAR(p) models). 

- Why VAR models exist, what they're better at than AR models. In predictions they don't just include a variable at time t, but take all previous observations into account.

### Model

(marko)
There are three main forms of VAR models: reduced form, recursive, and structural. The reduced form defines each variable in the vector of time series as the function of its own lags and the lags of all other variables, with an error term. The error terms from each equation can theoretically be correlated. Recursive VAR models are constructed so that the error terms for a variable are uncorrelated to the prior variables' errors. This is done by adding the  preceding variables' current values to the typical VAR equation. The structural VAR models are distinguished in that they make assumptions of the "causal structure" of the the data allow shocks to be indentified. These shocks would otherwise be incorporated into error terms in recursive and reduced models.

A reduced VAR(1) model with three variables can be modeled as such:

$$x_{t,1} = \alpha_1 + \beta_{1,1}x_{t-1,1} +\beta_{1,2}x_{t-1,2} + \beta_{1,3}x_{t-1,3}+\epsilon_{t,1}$$
$$x_{t,2} = \alpha_2 + \beta_{2,1}x_{t-1,1} +\beta_{2,2}x_{t-1,2} + \beta_{2,3}x_{t-1,3}+\epsilon_{t,2}$$
$$x_{t,1} = \alpha_1 + \beta_{3,1}x_{t-1,1} +\beta_{3,2}x_{t-1,2} + \beta_{3,3}x_{t-1,3}+\epsilon_{t,3}$$

- Generalized VAR

### Estimation

(marko)
- how its estimed

- "adding variables to the VAR creates complications, because the number of VAR parameters increases as the square of the number of variables: a nine-variable, four-lag VAR has 333 unknown coefficients" https://pubs.aeaweb.org/doi/pdf/10.1257/jep.15.4.101

- interpretation of coeffs

### Diagnostics
(matt)

### Forecasting

(marko)
- one step ahead

- recusive h-step ahead

### Data Example
(matt)

### Discussion

### References

### Code Appendix

```{r}
# reading in data

btc <- read_csv("BTC-USD.csv")
eth <- read_csv("ETH-USD.csv")
doge <- read_csv("DOGE-USD.csv")
eth <- eth[-1827, ] # matching length
```


```{r}
# creating closing price dataframe

closing_prices <- as.data.frame(cbind(btc$Close, eth$Close, doge$Close))

closing_prices$Date <- btc$Date

colnames(closing_prices) <- c("BTC", "ETH", "DOGE", "Date")

plot(closing_prices$Date, closing_prices$BTC, type = "l")
lines(closing_prices$Date, closing_prices$ETH, type = "l", col = "red")
lines(closing_prices$Date, closing_prices$DOGE, type = "l", col = "dark green")
```


```{r}
# creating scaled data dataframe

btc_scale <- scale(closing_prices$BTC)
doge_scale <- scale(closing_prices$DOGE)
eth_scale <- scale(closing_prices$ETH)

scaled_data <- data.frame(
  Date = closing_prices$Date,
  scaled_bth = btc_scale[, 1],
  scaled_eth = eth_scale[, 1],
  scaled_doge = doge_scale[, 1]
)

plot(scaled_data$Date, scaled_data$scaled_bth, type = "l")
lines(scaled_data$Date, scaled_data$scaled_eth, type = "l", col = "red")
lines(scaled_data$Date, scaled_data$scaled_doge, type = "l", col = "dark green")
```


```{r}
# creating scaled log dataframe
# resolving NAs
# na2lag1(c(closing_prices$BTC,closing_prices$DOGE,closing_prices$ETH))

Date <- closing_prices$Date
sl_btc <- scale(log((closing_prices$BTC)))[, 1]
sl_doge <- scale(log(as.numeric(closing_prices$DOGE)))[, 1]
sl_eth <- scale(log(as.numeric(closing_prices$ETH)))[, 1]
sl_data <- data.frame(Date, sl_btc, sl_doge, sl_eth)

plot(sl_data$Date, sl_data$sl_btc, type = "l")
lines(sl_data$Date, sl_data$sl_eth, type = "l", col = "red")
lines(sl_data$Date, sl_data$sl_doge, type = "l", col = "dark green")
```


```{r}
# differenced scaled (unlogged) data plot
plot(scaled_data$Date[c(-1, -2)], diff(scaled_data$scaled_bth, 2), type = "l")
lines(scaled_data$Date[c(-1, -2)], diff(scaled_data$scaled_eth, 2), type = "l", col = "red")
lines(scaled_data$Date[c(-1, -2)], diff(scaled_data$scaled_doge, 2), type = "l", col = "dark green")
```

```{r}
# varselect for scaled & logged data

slvar <- VARselect(na.omit(sl_data[,-1]), lag.max = 100, type = "both")
slvar$selection
plot(slvar$criteria[1,], type = 'l')
lines(slvar$criteria[2,], type = 'l', col = "red")
```



```{r}
# prepping data for vars package
scaled_data_clean <- scaled_data[, -1] %>% na.omit()
og_data_clean <- na.omit(closing_prices)
og_data_clean <- og_data_clean[, 4]

# fitting var model
fitvar <- VAR(scaled_data_clean, p = 2, type = "both")

summary(fitvar)
acf(residuals(fitvar))

# UNSURE; TO BE LABELED
ts_obj <- as.ts(scaled_data_clean)
var12 <- VAR(ts_obj, p = 12, type = "const")
serial.test(var12, lags.pt = 12, type = "PT.asymptotic")

# forecasts

forecast(var12) %>%
  autoplot() + xlab("Year")

# should we do an ADF test?? 

# granger test for causality COMMENTED OUT RN

  #grangertest(BTC ~ DOGE, data = og_data_clean)

  #grangertest(BTC ~ ETH, data = og_data_clean)

  #rangertest(DOGE ~ BTC, order = 50, data = og_data_clean)
```


```{r}

# removing NAs 
sl_data$sl_btc<-na2lag1(sl_data$sl_btc)
sl_data$sl_doge<-na2lag1(sl_data$sl_doge)
sl_data$sl_eth<-na2lag1(sl_data$sl_eth)


# using results of VARselect to fit two models than compare them
fitvar_p9 <- VAR(sl_data[,-1], p = 9, type = "both")
fitvar_p1 <- VAR(sl_data[,-1], p = 1, type = "both")

summary(fitvar_p9)
summary(fitvar_p1)

# diagnostic tests 

# Asymptotic Portmanteau test

# we fail to find evidence of autocorrelation in the residuals of the VAR(9) model at the default of 16 lags, but if we do 40 we find significant evidence of autocorrealtion, might be a problem??
serial.test(fitvar_p9, lags.pt = 40)

# we find evidence of autocorrelation in the residuals of the VAR(1) model
serial.test(fitvar_p1, lags.pt = 40)


# Breusch-Godfrey test

# same reversial present for the BG test...
serial.test(fitvar_p9,lags.bg = 40, type = 'BG')


serial.test(fitvar_p1,lags.bg = 40, type = 'BG')

# more diagnostics 
stablity_test_p9 <-stability(fitvar_p9, type = c("OLS-CUSUM"))
stablity_test_p1 <-stability(fitvar_p1, type = c("OLS-CUSUM"))

plot(stability_test_p9)
plot(stablity_test_p1)


# Jarqueâ€“Bera test is a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution
p9_normal <- normality.test(fitvar_p9)

plot(p9_normal)


p1_normal <- normality.test(fitvar_p1)

plot(p1_normal)

# Portmanteau Q and test for the null hypothesis that the residuals of a ARIMA model are homoscedastic
arch.test(fitvar_p9)
arch.test(fitvar_p1)


#making some forecasts

#need to refit model using slightly differntly formatted data 
sl_data_as_ts <- as.ts(sl_data[,-1])
fitvar_p9 <- VAR(sl_data_as_ts, p = 9, type = "both")

forecast(fitvar_p9, h = 20) %>%
  autoplot() + xlab("Year")

fitvar_p1 <- VAR(sl_data_as_ts, p = 1, type = "both")

forecast(fitvar_p1, h = 20) %>%
  autoplot() + xlab("Year")

# augmented dickey-fuller test unit root test

ur.df(sl_data$sl_btc %>% na.omit(), lags = 2) %>% summary()
```


