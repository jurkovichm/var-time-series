---
title: "Vector Autoregressive Models"
subtitle: "Time Series Final | 'Flavor 2'"
author: "Marko Jurkovich & Matt Zacharski"
date: "7 June 2021"
output: pdf_document
abstract: "Vector Autoregresive Models or VAR models, are an extension of AR models used to analyzed multivariate time series. Each variable is a linear combination of its own lags combined with the lags of other variables. We show how VAR models can be used to analyze several cryptocurrency and cover several common methods associated with estimating and forecasting multiple time series. We find that a tri-variate VAR(9) can potentially be used to adequately predict crypto-currency data over very short time periods."
keywords: "VAR, time series, Granger-causality test, cryptocurrency, bitcoin" 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(vars)
library(lmtest)
library(forecast)


# na 'resolver'. turns na's into previous day's close value

na2lag1 <- function(x) {
  i <- 1
  while (i <= length(x)) {
    if (is.na(x[i])) {
      x[i] <- x[i - 1]
    }
    i <- i + 1
  }
  return(x)
}
```


\newpage

### Introduction

The vector autoregressive model (VAR) is a time series model that is typically used for the analysis of multivariate time series. Consequently, VAR models are particularly useful in fields where several time series may affect one another. For example, a researcher may be interested in exploring the relationship between several foreign exchange rates overtime or the trends of two countries' GDPs. While the fields of economics and finance may be initial fields that come to mind when discussing VAR models, the natural sciences also use VAR models to analysis and forecast things such as rainfall while also considering humidity, temperature, and other factors. 

Like the name suggests, VAR models are an extension of the univariate autoregressive model. Similar to the univariate autoregressive model, each variable's equation includes its own lagged values. However, the VAR model goes a step further and also includes lagged values of other time series in each variable's equation. Including this extra information allows VAR models to often make more accurate predictions than the general AR model, justifying their existence and use. It is worth noting that several related models have been developed over the past few decades. Namely, structural vector autoregressive models, vector error correction models, and structural vector error correction models. While these models are outside the scope of this paper, it is useful to know they attempt to improve the VAR model by accounting for concepts such as cointegration or some type of shock. 

In this paper, we will discuss the form, estimation, diagnostics, and forecasting process of VAR models. To highlight these methods, we have included a brief analysis of three cryptocurrency time series. Additionally, this paper will only deal with the selection and forecasting of stationary VAR models. 


### Model

The basic structure of a VAR model is quite similar to that of an AR model, however there are two fundamental differences. The first is that there are multiple equations, one for each item in the vector of time series considered. Secondly, rather than have a time point $x_t$ of a given time series being regressed on its own lags, it is also regressed on the lags of the other time series. The order of the model (number of lags) is typically indicated by the *p* in VAR(*p*).

There are three main forms of VAR models: reduced form, recursive, and structural. The reduced form defines each variable in the vector of time series as the function of its own lags and the lags of all other variables, with an error term. The error terms from each equation can theoretically be correlated, *but only within the same time period*. Correlation across equations across different time periods would imply autocorrelated errors within a single time series, which is not an assumption of the model.

Recursive VAR models are constructed so that the error terms for a variable are uncorrelated to the prior variables' errors. This is done by adding the  preceding variables' current values to the typical VAR equation. 

The structural VAR models are distinguished in that they make assumptions of the "causal structure" of the the data allow shocks to be indentified. These shocks would otherwise be incorporated into error terms in recursive and reduced models.

A reduced VAR(1) model with  three variables can be modeled as such:

\begin{equation}
\begin{aligned}
x_{t,1} = \alpha_1 + \beta_{1,1}x_{t-1,1} +\beta_{1,2}x_{t-1,2} + \beta_{1,3}x_{t-1,3}+\epsilon_{t,1} \\
x_{t,2} = \alpha_2 + \beta_{2,1}x_{t-1,1} +\beta_{2,2}x_{t-1,2} + \beta_{2,3}x_{t-1,3}+\epsilon_{t,2} \\
x_{t,1} = \alpha_3 + \beta_{3,1}x_{t-1,1} +\beta_{3,2}x_{t-1,2} + \beta_{3,3}x_{t-1,3}+\epsilon_{t,3}
\end{aligned}
\end{equation}

We can generalize the previous equations for a VAR(*p*) model with *k* variables using matrix algebra:

\begin{equation}
\begin{bmatrix}x_{1,t} \\ x_{2,t}\\ \vdots \\ x_{k,t}\end{bmatrix} = 
\begin{bmatrix} \alpha_{1}^1 & \alpha_{1}^2 \\ \alpha_{2} & \alpha_{2}^2\\ \vdots & \vdots  \\ \alpha_{k}^1 & \alpha_k^2\end{bmatrix} \begin{bmatrix} 1 \\ t \end{bmatrix}+
\begin{bmatrix}
\beta_{1,1}^1 & \cdots & \beta_{1,k}^1\\
\beta_{2,1}^1 & \cdots & \beta_{2,k}^1\\
\vdots& \ddots& \vdots\\
\beta_{k,1}^1& \cdots & \beta_{k,k}^1
\end{bmatrix}
\begin{bmatrix}x_{t-1,1} \\ x_{t-1,2}\\ \vdots \\ x_{t-1,k}\end{bmatrix}
+ \cdots +
\begin{bmatrix}
\beta_{1,1}^p & \cdots & \beta_{1,k}^p\\
\beta_{2,1}^p & \cdots & \beta_{2,k}^p\\
\vdots& \ddots& \vdots\\
\beta_{k,1}^p & \cdots & \beta_{k,k}^p
\end{bmatrix}
\begin{bmatrix}y_{t-p,1} \\ y_{t-p,2}\\ \vdots \\ y_{t-p,k}\end{bmatrix}
+ \begin{bmatrix}\epsilon_{t,1} \\ \epsilon_{t,2}\\ \vdots \\ \epsilon_{t,3}\end{bmatrix}
\end{equation}

or, abstracting the matrices

\begin{equation}
\bf{x}_{t} =Au + B^1x_{t-1} +...+B^px_{t-p} +\epsilon_t 
\end{equation}

where $\bf{x}_{t}$ is an (k x 1) vector of time series variables, and $\bf{x}_{t-i}$ are the vectors of the time series variables at various lags determined by *p*. $\bf{A}$ is the coefficient matrix on the constant and trend term (both optional) represented in $\bf{u}$. $\bf B^ix_{t-1}$ are the (k x k) coefficient matrices; there are *p* of these matrices dependent on the order of the VAR(p) model. $\bf \epsilon_t$ is the vector of error terms.

### Estimation

VAR models fall into the category of Seemingly Unrealted Regression (SUR) models, which, without diving into too much detail, allows the combination multivariate systems into a conceptially simpler notation. The VAR case of SUR models in one in which all the explanatory variables (but not the coefficients, importantly) are indentical across equations. While SUR models often use Generalized Least Squares to estimate the equations, in the case of VAR models OLS is theoretically just as efficient at estimating each equation individually.

An extremely important consideration before estimating VAR models however, is parameter selection and extent. As both variables and lags are added to VAR models, the number of coefficients increases quadratically. For example, a "nine-variable, four-lag VAR has 333 unknown coefficients" (Stock 110).

- interpretation of coeffs

- asymptotically valid t-tests on individual coefficients may be constructed in the usual way

### Diagnostics and Lag Length Selection


The diagnostic testing for VAR models is similar to the diagnostic testing for AR models except with a few extra tests to consider. **Idk if we want to include information criteria in the diagnostic section or not but it seems like something worth considering**  

However, first we will go over the lag length selection process. The question of how many lag terms to include in a VAR model is an important because it plays a substantial role in the quality of forecasts the model will make. The common approach to select this number is to fit VAR(*p*) models with lag lengths from *p* = 0,...,$p_{max}$ and choose the *p* with that minimizes the considered information criteria. Typically, Akaike, Bayesian, Hannan-Quinn, and forecast prediction error information criterion are considered. Model selection criteria for VAR(*p*) models has the form

\begin{equation}
\begin{aligned}
IC(p)=ln|\tilde \sum(p)|+c_T*\varphi(n,p)
\end{aligned}
\end{equation}

where $\tilde \sum(p) = T^-1 \sum^{T}_{t=1} \hat \epsilon_t \hat \epsilon^\prime_t$ represents the residual covariance matrix without a degrees of freedom correction, $c_T$ is a sequence indexed by the sample size T, and $\varphi(n,p)$ is a penalty function to penalize large VAR(*p*) models.*neccessary to write out the individual equations for each criteria?? idk*. Each criterion has its benefits and drawbacks. However, these are not the focus on this paper but it is generally best to consider several criteria when determining the optimal lag length selection. 

It is also beneficial to consider running Granger Causality tests on several or all of the variables that you are considering including in your VAR model. A variable is said to granger-cause another variable if the first variable is found to be a useful predictor of the second variable. Essentially, a variable $x_1$ fails to granger-cause $x_2$ if for all s>0 the MSE of a forecast of $x_{2,t+s}$ based on lags of $x_2$ is the same or less than a forecast of $x_{2,t+s}$ based on lags of $x_2$ and lags of $x_1$. Notably, granger causality does not say anything about true causality, only about forecasting ability. 


When determining the quality of the fit of a certain VAR model, it is useful to consider the ACF and PACFs of the residuals. Similar to evaluating an AR model, we are looking for ACF and PACF plots that show no Considering autocorrealtion within the residuals. Unlike with the univariate AR models, we must also consider cross correaltion plots. Considering the ACF plots of the residuals for each set of residuals of all of the variables in a VAR model is important in order to obsere any cross-correlations between variables indicating that your model might not be capturing the relationship between those variables and time sufficiently. 

To test for heteroscedasticity within errors it is common to use an ARCH-LM test. However, this test relies on a complicated regression that is outside the scope of this paper. Similarly, the technical details of the normality test are also outside the scope of this paper but the gist of it is that researchers should use Jarque-Bera normality tests to test for normality of residuals of either each individual equation or the normality of all the equations' residuals using a variance-covariance matrix for the centered residuals.

It is also wise to test for autocorrelation within the residuals of VAR(*p*) models to avoid standard errors being too small or T-statistics that are too large. Do to this within a VAR(*p*) setting we can conduct a Portmanteau test of the form

\begin{equation}
\begin{aligned}
Q_h = T \sum_{j = 1}^h tr(\hat{C}_j'\hat{C}_0^{-1}\hat{C}_j\hat{C}_0^{-1}) \quad
\end{aligned}
\end{equation}

where $\hat{C}_i = \frac{1}{T}\sum_{t = i + 1}^T \bf{\hat{u}}_t \bf{\hat{u}}_{t - i}'^\top$ and the test statistics $Q_h$ has an approximate $\chi^2(K^2(h - n^*))$ distribution. $n^*$ refers to the number of non-deterministic terms in the specified VAR(p) model.

To test for the stability of a VAR model coefficients overtime, it is typical to compute an empirical fluctuation processes of type OLS-CUSUM.


### Forecasting 

Forecasting in VAR is somewhat straightforward and similar to typical AR models. A one-step ahead forecast made at time *t+1* can be expressed as:

\begin{equation}
\bf{x}_{t+1|t} =Au + B^1x_t +...+B^px_{t-p+1}
\end{equation}

Predicting more than one step ahead is done recursively in what can be referred to as the chain-rule of forecasting:

\begin{equation}
\bf{x}_{t+1|t} =Au + B^1x_t +...+B^px_{t-p+1}
\end{equation}

- what happens to SEs

### Data Example

Give a brief introduction of an example dataset and show how the methods
you’ve discussed can be used in data analysis. (You may use textbook data or a simulation
here.)

As mentioned in the introduction, VAR models are commonly used by the field of finance in order to explore the relationships between several time series and to use the extra information provided by multivariate series in forecasting. Consequently, we downloaded three time series of daily closing prices for Doge coin, Bitcoin, and Ethereum for the past 5 years - from 2016-05-26 to 2021-05-25. While cryptocurrnecies never 'close' in the traditional stock exchange sense, closing price typically refers to the price at 11:59PM UTC on any given day. 

![Scaled and Logged Series Plot](scaledloged.png){#id .class width=50% height=50%}

Figure 1 shows a plot of each series after they have been scaled and logged to adjust for differences in scale and non-stationary behavior. Running the *VARselect* command we see that two of the selection criteria, AIC and FPE, suggest a *p* = 9 and the other two criteria, HQ and SC, suggest a *p* = 1. Consequently, we will fit two VAR models with each respective value of *p*. We also run several granger causality tests to see if any of the series appear to be useful in predicting the other at an order of 1. Unfortunately, we find that the only currency pairing that was significantly predictive was BTC of DOGE coin.

![p9 cross correlation](residuals_p9.png){#id .class width=50% height=50%} ![p1 cross correaltion](residuals_p1.png){#id .class width=50% height=50%}

The grid of nine plots to the left is the cross correaltion plot for the fit with *p* = 9 and the grid to the right is fit with *p* = 1. Ideally all these plots would like like white noise. However, we notice that for both models the residuals of ETH and BTC seem to show significant autocorrelation at lag 0 as well as the DOGE and BTC residuals. 

Conducting a Asymptotic Portmanteau test, we find that we fail to find significant evidence of autocorrealtion in the residuals for the p9 model but we do for the p1 model. Additionally, we conducted a stability test for each fit and found all equations across each model to be relatively stable. Lastly, the fits appear to have normally distributed errors but according to a ARCH Engle's test for residual heteroscedasticity it appears both models contain evidance of heteroscedasticitiy in their residuals which is not ideal.

Despite some not ideal diagnostic tests, we decided to use the p9 model to forecast some data due to its lack of autocorrelation found during the Asymptotic Portmanteau test and its slightly better looking stability test plots. 

![Forecast Using VAR(9)](forecast.png){#id .class width=50% height=50%}


Additionally, we also collected the most recent closing prices for the cryptocurrencies since we had collected the data on 2021-05-25 and compared the forecasted VAR(9) values to the actual data values for each currency. 

![Forecast Using VAR(9)](forecastBTC.png){#id .class width=50% height=50%} ![Forecast Using VAR(9)](forecastETH.png){#id .class width=50% height=50%}
![Forecast Using VAR(9)](forecastDOGE.png){#id .class width=50% height=50%}

The mean percentage difference between the forecasted data and the actual data for BTC, ETH, and DOGE coin is 2.5%, 2.62%, and 3.93% respectively. Since the data has been scaled and logged to such a degree, interpretation of the actual forecasts is difficult. However, we are glad to see our model performing somewhat accurately over a very short term. 


### Discussion

seems like there's several areas of analysis when it comes to VAR models -- forecasting then observing the relationship between variables etc 

- other types of models (VAR is basic, and we use a basic version of it.)

- granger causality isnt very supportive of the use of the VAR except in the case of BTC -> DOGE. Which makes sense. Variable are cointegrated which may mean time series w common causal effect need addition consideration or seperate models.

- importance of assumptions

### References

Pfaff, Bernhard. VAR, SVAR and SVEC Models: Implementation Within R Package vars. Journal of Statistical Software 27(4), 2008. https://cran.r-project.org/web/packages/vars/vignettes/vars.pdf.

PennState Eberly College of Science. Vector autoregressive models VAR(p) models.
https://online.stat.psu.edu/stat510/lesson/11/11.2.

Shumway, Robert H., and David S. Stoffer. Time Series Analysis and Its Applications. Springer Texts in Statistics, Springer International Publishing, 2017, pp. 273–279. https://link.springer.com/content/pdf/10.1007%2F978-3-319-52452-8.pdf.

Stock, James H., and Mark W. Watson. “Vector Autoregressions.” Journal of Economic Perspectives, vol. 15, no. 4, American Economic Association, Nov. 2001, pp. 101–15. Crossref, doi:10.1257/jep.15.4.101.
